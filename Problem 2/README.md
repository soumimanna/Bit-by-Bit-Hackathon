## ModelSpy: Identifying CNN Models from CPU Side-Channel Traces
This repository contains the complete solution for the "ModelSpy" challenge from the Bit-by-Bit Side Channel Hackathon (September 21, 2025).
The project demonstrates how to build a classification pipeline to identify a running CNN model by analyzing CPU performance counter data collected via a 
side channel.

---
### The Challenge: Model-in-the-Middle
In a multi-tenant cloud environment, CPU resources are shared. While partitions are in place, system-level telemetry like cache misses, instruction counts,
and branch prediction data can leak information about the workloads being run by other users. These "side-channel fingerprints" are often unique enough to 
identify specific applications.

Our goal is to create a machine learning model that can accurately classify which of the following seven CNN models is running, 
based solely on these leaked performance counter traces:
Resnet, AlexNet, VGG,DenseNet,Inception_V3 ,MobileNet_V2 ,ShuffleNet_V2

---
### Methodology & Workflow
The solution is broken down into a three-step pipeline:

Trace Collection: Systematically collect raw performance counter data for each of the target CNN models.

Feature Engineering: Process the raw, time-series data into a structured format by calculating statistical features.

Leakage Analysis & Visualization: Analyze the processed features to determine if the models have unique fingerprints and train a classifier to
distinguish between them.

---
### Step 1: Collect Traces
The collect_traces.sh script uses perf stat to monitor a running process (model_inference) and record performance counter data at a 50ms interval.

Script Configuration:

MODELS: An array of the seven target CNN models.

EVENTS: The specific CPU performance counters to monitor (e.g., cycles, cache-misses).

NUM_RUNS: The number of times to repeat the collection for each model/event pair (default is 5).

OUTPUT_DIR: The directory where the raw trace files will be saved (my_collected_traces2/).

To run the collection, execute the script from your terminal:

bash collect_traces.sh

This will create a directory structure like my_collected_traces2/Resnet/cycles_run1.txt.

### Step 2: Process Traces and Engineer Features
After collection, the process_traces.py script is used to convert the raw text files into a structured CSV file.

What it does:

Parses Data: It reads each .txt file and extracts the time-series of performance counter values.

Calculates Features: For each time-series, it calculates key statistical features: mean, std, median, min, max, and variance.

Creates Dataset: It aggregates these features into a single features.csv file, where each row represents one run of a model, and the columns represent the 
statistical features for each event (e.g., cycles_mean, cache-misses_std).

Run the script using Python:

python process_traces.py

### Step 3: Analyze and Visualize the Leakage
The final script, analyze_leakage.py, uses the features.csv file to explore the data and verify that the models are distinguishable.

Key Analyses:

PCA Visualization: It performs Principal Component Analysis (PCA) to reduce the high-dimensional feature space to 2D. This allows us to visually
inspect how well the different model fingerprints cluster and separate. The output is saved as pca_model_separation.png.

Prediction Confidence Analysis: It trains a RandomForestClassifier on the features and calculates the model's average prediction confidence for each class. This helps quantify how "easy" or "hard" it is to identify each model.

Execute the analysis script:

python analyze_leakage.py

After running, you will have a pca_model_separation.png image and a console output showing the average prediction confidence for each CNN model.

The PCA plot generated by the analysis script demonstrates clear clustering for the different CNN models, confirming that they each have a distinct side-channel fingerprint based on the selected performance counters. The confidence analysis further quantifies this separation, showing that a standard classifier can distinguish between the models with high accuracy.

This project successfully demonstrates the viability of using CPU side-channel data to identify specific workloads in a shared-resource environment.
